{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7072e34f3265d9f952c8c33f0c61239ec1c23d1d"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport re\nimport operator \nimport string\nfrom tqdm import tqdm\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm\nimport nltk\nfrom imblearn.over_sampling import SMOTE\nimport itertools\nimport re\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english')) \nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"488864b648d7143d220acb755d15d5e3980f042f"},"cell_type":"code","source":"def clean_text(x):\n    x = str(x)\n    for p in string.punctuation + '“…':\n        x = x.replace(p, ' ' + p + ' ')\n    \n    x = x.replace('_', '')\n    \n    x = re.sub(\"`\",\"'\", x)\n    x = re.sub(\"(?i)n\\'t\",' not', x)\n    x = re.sub(\"(?i)\\'re\",' are', x)\n    x = re.sub(\"(?i)\\'s\",' is', x)\n    x = re.sub(\"(?i)\\'d\",' would', x)\n    x = re.sub(\"(?i)\\'ll\",' will', x)\n    x = re.sub(\"(?i)\\'t\",' not', x)\n    x = re.sub(\"(?i)\\'ve\",' have', x)\n    x = re.sub(\"(?i)\\'m\",' am', x)\n    \n    x = re.sub(\"(?i)n\\’t\",' not', x)\n    x = re.sub(\"(?i)\\’re\",' are', x)\n    x = re.sub(\"(?i)\\’s\",' is', x)\n    x = re.sub(\"(?i)\\’d\",' would', x)\n    x = re.sub(\"(?i)\\’ll\",' will', x)\n    x = re.sub(\"(?i)\\’t\",' not', x)\n    x = re.sub(\"(?i)\\’ve\",' have', x)\n    x = re.sub(\"(?i)\\’m\",' am', x)\n    \n    x = re.sub('(?i)Quorans','Quora', x)\n    x = re.sub('(?i)Qoura','Quora', x)\n    x = re.sub('(?i)Quoran','Quora', x)\n    x = re.sub('(?i)dropshipping','drop shipping', x)\n    x = re.sub('(?i)HackerRank','Hacker Rank', x)\n    x = re.sub('(?i)Unacademy','un academy', x)\n    x = re.sub('(?i)eLitmus','India hire employees', x)\n    x = re.sub('(?i)WooCommerce','Commerce', x)\n    x = re.sub('(?i)hairfall','hair fall', x)\n    x = re.sub('(?i)marksheet','mark sheet', x)\n    x = re.sub('(?i)articleship','article ship', x)\n    x = re.sub('(?i)cryptocurrencies','cryptocurrency', x)\n    x = re.sub('(?i)coinbase','cryptocurrency', x)\n    x = re.sub('(?i)altcoin','bitcoin', x)\n    x = re.sub('(?i)altcoins','bitcoins', x)\n    x = re.sub('(?i)litecoin','bitcoin', x)\n    x = re.sub('(?i)litecoins','bitcoins', x)\n    x = re.sub('(?i)demonetisation','demonetization', x)\n    x = re.sub('(?i)ethereum','bitcoin', x)\n    x = re.sub('(?i)ethereums','bitcoins', x)\n    x = re.sub('(?i)quorans','quora', x)\n    x = re.sub('(?i)Brexit','britan exit', x)\n    x = re.sub('(?i)upwork','freelance', x)\n    x = re.sub('(?i)Unacademy','un academy', x)\n    x = re.sub('(?i)Blockchain','blockchain', x)\n    x = re.sub('(?i)GDPR','General Data Protection Regulation', x)\n    x = re.sub('(?i)Qoura','quora', x)\n    x = re.sub('(?i)HackerRank','Hacker Rank', x)\n    x = re.sub('(?i)Cryptocurrency','cryptocurrency', x)\n    x = re.sub('(?i)Binance','cryptocurrency', x)\n    x = re.sub('(?i)Redmi','mobile phone', x)\n    x = re.sub('(?i)TensorFlow','Tensor Flow', x)\n    x = re.sub('(?i)Golang','programming language', x)\n    x = re.sub('(?i)eLitmus','India hire employees', x)\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fd76252faf1da89392131be5e1f3ff9ebb0b0f4"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')\nprint('Train shape : ', train_df.shape)\nprint('Test shape : ', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a079e83c3e238fd9db1c4c794f97b671aef7b01f"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_df, test_size=0.2,random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a90693765780b4eb287287ab899928c6e0e9d1d"},"cell_type":"code","source":"from tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas(desc=\"Example Desc\")\ntrain_df['question_text'] = train_df['question_text'].progress_apply(lambda x: clean_text(x))\nval_df['question_text'] = val_df['question_text'].progress_apply(lambda x: clean_text(x))\ntest_df['question_text'] = test_df['question_text'].progress_apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d1b9fad72d9f3ef29a5e439a4e5cab896144b28"},"cell_type":"code","source":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9f709d190315c7274974756452955d07886ac75"},"cell_type":"code","source":"train_df['question_text'] = train_df['question_text'].progress_apply(lambda x: clean_text(x))\nval_df['question_text'] = val_df['question_text'].progress_apply(lambda x: clean_text(x))\ntest_df['question_text'] = test_df['question_text'].progress_apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8e87b5f2002d693209f94b14f025dc01cded82f"},"cell_type":"code","source":"import re\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03294b4d058f9dfd9d02952268ebdc40e0c98412"},"cell_type":"code","source":"train_df['question_text'] = train_df['question_text'].progress_apply(lambda x: clean_text(x))\nval_df['question_text'] = val_df['question_text'].progress_apply(lambda x: clean_text(x))\ntest_df['question_text'] = test_df['question_text'].progress_apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0316a0fe47344412d4d1db7226e4f920c1102424"},"cell_type":"code","source":"def _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'\n\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2d7e193d3cde3deaad3c903923639590df455dc"},"cell_type":"code","source":"train_df['question_text'] = train_df['question_text'].progress_apply(lambda x: replace_typical_misspell(x))\nval_df['question_text'] = val_df['question_text'].progress_apply(lambda x: replace_typical_misspell(x))\ntest_df['question_text'] = test_df['question_text'].progress_apply(lambda x: replace_typical_misspell(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf333c1d4a67ea30da85379eb9febd1d809996a6"},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\n          \ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ntrain_df['question_text'] = train_df['question_text'].str.lower()\nval_df['question_text'] = val_df['question_text'].str.lower()\ntest_df['question_text'] = test_df['question_text'].str.lower()\n    \ntrain_df['question_text'] = train_df['question_text'].progress_apply(lambda x: clean_text(x))\nval_df['question_text'] = val_df['question_text'].progress_apply(lambda x: clean_text(x))\ntest_df['question_text'] = test_df['question_text'].progress_apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"101a42f068ff42e01425b3b40ed24a7db8df3252"},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f940c66dcfe70bffe0786cf68f743b26d1c15e32"},"cell_type":"code","source":"full_text = list(train_df['question_text'].values) + list(val_df['question_text'].values)+list(test_df['question_text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4b8e1c6f076cbd402c7a9f2ffd6ee06591f099a"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport keras \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, BatchNormalization\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam, RMSprop\n\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d4458ff609cccd57e39b4a6484a4abb1d6caa82"},"cell_type":"code","source":"tokenizer = Tokenizer(lower = True, filters = '')\ntokenizer.fit_on_texts(full_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bca7b8a061c2af37fcf992aa4f7bb0571199158c"},"cell_type":"code","source":"train_tokenized = tokenizer.texts_to_sequences(train_df['question_text'])\nval_tokenized = tokenizer.texts_to_sequences(val_df['question_text'])\ntest_tokenized = tokenizer.texts_to_sequences(test_df['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dd5eb046ec391a0d917b09b5aaf01d23cef5a47"},"cell_type":"code","source":"max_len = 70\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_val = pad_sequences(val_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2715aa0a9759952826d4254bb5c83d80ed069bc1"},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4a5f7d9eba646ab688b150d8e18572610a30cc7"},"cell_type":"code","source":"X_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"385b762934941e8b6be5ad1039f4158ad1dd7edd"},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a08a1152a855d2355bdb5cc50a539844bfb772"},"cell_type":"code","source":"print(os.listdir(\"../input/embeddings\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4dd2e10ad848a1dc78cb0562406b1bb83a14f89"},"cell_type":"code","source":"embedding_path = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"130bccca77dd59585c47c4b0b1b0903343f4f770"},"cell_type":"code","source":"embed_size = 300\nmax_features = 100000 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06027a307bba7b35bc22edec8bb6a72fac0aa959"},"cell_type":"code","source":"def get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ef95c38957cd768a2d3eaf0e66378ddf3b3aacd"},"cell_type":"code","source":"def get_embed_mat(embedding_path):\n    \n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\n    word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n        \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d045a0c241bef0efe7cf6f892c03e94d76c4f1"},"cell_type":"code","source":"y = train_df['target']\n\n#one_hot_encoder = OneHotEncoder(sparse=False)\n#y_one_hot = one_hot_encoder.fit_transform(y.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5717840db0caee22188549e61c17045320bac84"},"cell_type":"code","source":"y_val=val_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"017243b777b0f478532bab8f95515a6481680a20"},"cell_type":"code","source":"file_path = \"model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e16ff7545155e504b10b797b685879095bc6316e"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b555eb742ee24749a8ae5487e8d8be20e89b8719"},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b35fe883e9d2065df9590f6388a93f6b888260f"},"cell_type":"code","source":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = Embedding(100001, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Attention(max_len)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n    history = model.fit(X_train, y, batch_size = 256, epochs = 3, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    #model = load_model(file_path)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ffd45f437aaa25b5894a0506138d3cd9259a470"},"cell_type":"code","source":"embedding_matrix = get_embed_mat(embedding_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c506d9881b8cc60175167ad3df41f74899818b5a","scrolled":true},"cell_type":"code","source":"model = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5a225724df3c94a9a54554bec6e3888d61a1390"},"cell_type":"code","source":"#from keras.models import load_model\n#model = load_model(\"../input/quora-new/model.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28be86088ba94a285afaae8a52ba8239bef37542"},"cell_type":"code","source":"pred = model.predict(X_val, batch_size = 1024, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dab825e5afbfb8c6badaab0dfd5df2a4a108005"},"cell_type":"code","source":"predictions = (pred > 0.35).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b85d15c3c5ce62359d99d5fdcfebd91124873f9"},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"439e9c949b27ea70721778537e7dd9e3a4b45735"},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.accuracy_score(y_val,predictions))\nprint(metrics.f1_score(y_val,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f49ede09cdf1418abb5f822ee1fcdb6a4787f8fe"},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd6bb7e075c376631ffde6682b7489ff86a456e9"},"cell_type":"code","source":"pred = model.predict(X_test, batch_size = 1024, verbose = 1)\npredictions = (pred > 0.37).astype(int)\nsub['prediction'] = predictions\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbe89b335400dee50878f858aec50aea2d6787ff"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}